{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MatrixNetworks\n",
    "using Distributions\n",
    "using Gadfly\n",
    "using Clustering\n",
    "using BenchmarkTools, Compat\n",
    "using DelimitedFiles\n",
    "using SparseArrays\n",
    "using LinearAlgebra\n",
    "using SphericalGraphEmbedding\n",
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = \"PolBlogs\"\n",
    "dataset = \"PowerEU\"\n",
    "dataset = \"Facebook\"\n",
    "dataset = \"PowerUS\"\n",
    "dataset = \"HEP\"\n",
    "# dataset = \"Internet\"\n",
    "# dataset = \"CondMat2003\"\n",
    "#dataset = \"Pokec\"\n",
    "\n",
    "# Initialization\n",
    "A = 0\n",
    "r = 0\n",
    "dim0 = 0\n",
    "community = zeros(Int64,N,1);\n",
    "\n",
    "# nb iterations projected power method\n",
    "n_it_PPM = 30000\n",
    "\n",
    "# relative objective variation\n",
    "t = 1e-8\n",
    "\n",
    "# nb of times vector partition is repeated\n",
    "n_it_vec_part = 10 \n",
    "\n",
    "# nb updates vec part\n",
    "n_updates = 50 \n",
    "\n",
    "# shape of embedding\n",
    "shape = \"Ellipsoidal\"\n",
    "\n",
    "# Loading data\n",
    "\n",
    "if dataset == \"PolBlogs\"\n",
    "\n",
    "    A0 = readdlm(\"data/PolBlogs/PolBlogsAdj.txt\")\n",
    "    lab = readdlm(\"data/PolBlogs/LabelsPolBlogs.txt\")\n",
    "\n",
    "    dim0 = size(A0)\n",
    "    A  = zeros(Int64 ,dim0[1],dim0[1])\n",
    "\n",
    "    for i = 1:dim0[1]\n",
    "        for j = 1:dim0[1]\n",
    "            A[i,j] = Int64(A0[i,j])\n",
    "        end\n",
    "    end\n",
    "    A = A+A'\n",
    "    A = sparse(A)\n",
    "\n",
    "    A,_ = largest_component(A)\n",
    "\n",
    "    A0 = 0\n",
    "    N = size(A,2)\n",
    "\n",
    "    # number of columns of initial guess\n",
    "    d0 = 20\n",
    "    # number of centroids thrown in embedding\n",
    "    n_clusters = 100\n",
    "\n",
    "elseif dataset == \"PowerEU\"\n",
    "\n",
    "    list = readdlm(\"data/PowerEU/ElistPowerEU.txt\")\n",
    "    dim0 = size(list)\n",
    "\n",
    "    M_int = zeros(Int64,dim0[1],2)\n",
    "\n",
    "    for i = 1:dim0[1]\n",
    "        for j = 1:2\n",
    "            M_int[i,j] = Int64(list[i,j])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    N = maximum([maximum(M_int[:,1]),maximum(M_int[:,2])])\n",
    "\n",
    "    A0 = spzeros(N,N)\n",
    "    A0 = sparse(M_int[:,1],M_int[:,2],vec(ones(Int64,dim0[1],1)),N,N)\n",
    "    A0 = A0 + A0'\n",
    "    A,_ = largest_component(A0)\n",
    "\n",
    "    A0 = 0\n",
    "    M_int = 0\n",
    "\n",
    "    # number of columns of initial guess\n",
    "    d0 = 50\n",
    "    # number of centroids thrown in embedding\n",
    "    n_clusters = 100\n",
    "\n",
    "elseif dataset == \"Facebook\"\n",
    "\n",
    "    list = readdlm(\"data/Facebook/facebook-gender_edges.txt\")\n",
    "    dim0 = size(list)\n",
    "\n",
    "    M_int = zeros(Int64,dim0[1],2)\n",
    "\n",
    "    for i = 1:dim0[1]\n",
    "        for j = 1:2\n",
    "            M_int[i,j] = Int64(list[i,j]) + 1 # here start at 0\n",
    "        end\n",
    "    end\n",
    "\n",
    "    N = maximum([maximum(M_int[:,1]),maximum(M_int[:,2])]) # here start at 0\n",
    "\n",
    "    A0 = spzeros(N,N)\n",
    "    A0 = sparse(M_int[:,1],M_int[:,2],vec(ones(Int64,dim0[1],1)),N,N)\n",
    "    A0 = A0 + A0'\n",
    "    A,_ = largest_component(A0)\n",
    "\n",
    "    A0 = 0\n",
    "    M_int = 0\n",
    "\n",
    "    # number of columns of initial guess\n",
    "    d0 = 50\n",
    "    # number of centroids thrown in embedding\n",
    "    n_clusters = 100\n",
    "\n",
    "elseif dataset == \"PowerUS\"\n",
    "\n",
    "    list = readdlm(\"data/PowerUS/ElistPowerUS.txt\")\n",
    "    dim0 = size(list)\n",
    "\n",
    "    M_int = zeros(Int64,dim0[1],2)\n",
    "\n",
    "    for i = 1:dim0[1]\n",
    "        for j = 1:2\n",
    "            M_int[i,j] = Int64(list[i,j])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    N = maximum([maximum(M_int[:,1]),maximum(M_int[:,2])])\n",
    "\n",
    "    A0 = spzeros(N,N)\n",
    "    A0 = sparse(M_int[:,1],M_int[:,2],vec(ones(Int64,dim0[1],1)),N,N)\n",
    "    A0 = A0 + A0'\n",
    "    A,_ = largest_component(A0)\n",
    "\n",
    "    A0 = 0\n",
    "    M_int = 0\n",
    "\n",
    "    # number of columns of initial guess\n",
    "    d0 = 50\n",
    "    # number of centroids thrown in embedding\n",
    "    n_clusters = 100\n",
    "\n",
    "elseif dataset == \"HEP\"\n",
    "\n",
    "    list = readdlm(\"data/HEP/hep-year_edges.txt\")\n",
    "    dim0 = size(list)\n",
    "\n",
    "    M_int = zeros(Int64,dim0[1],2)\n",
    "\n",
    "    for i = 1:dim0[1]\n",
    "        for j = 1:2\n",
    "            M_int[i,j] = Int64(list[i,j]) \n",
    "        end\n",
    "    end\n",
    "\n",
    "    N = maximum([maximum(M_int[:,1]),maximum(M_int[:,2])]) \n",
    "\n",
    "    A0 = spzeros(N,N)\n",
    "    A0 = sparse(M_int[:,1],M_int[:,2],vec(ones(Int64,dim0[1],1)),N,N)\n",
    "    A0 = A0 + A0'\n",
    "    A,_ = largest_component(A0)\n",
    "\n",
    "    A0 = 0\n",
    "    M_int = 0\n",
    "\n",
    "    # number of columns of initial guess\n",
    "    d0 = 50\n",
    "    # number of centroids thrown in embedding\n",
    "    n_clusters = 100\n",
    "\n",
    "elseif dataset == \"Wikivote\"\n",
    "\n",
    "    list = readdlm(\"data/Wikivote/wiki-Vote.txt\")\n",
    "    dim0 = size(list)\n",
    "\n",
    "    M_int = zeros(Int64,dim0[1],2)\n",
    "\n",
    "    for i = 1:dim0[1]\n",
    "        for j = 1:2\n",
    "            M_int[i,j] = Int64(list[i,j]) \n",
    "        end\n",
    "    end\n",
    "\n",
    "    N = maximum([maximum(M_int[:,1]),maximum(M_int[:,2])]) \n",
    "\n",
    "    A0 = spzeros(N,N)\n",
    "    A0 = sparse(M_int[:,1],M_int[:,2],vec(ones(Int64,dim0[1],1)),N,N)\n",
    "    A0 = A0 + A0'\n",
    "    A,_ = largest_component(A0)\n",
    "\n",
    "    A0 = 0\n",
    "    M_int = 0\n",
    "\n",
    "    # number of columns of initial guess\n",
    "    d0 = 50\n",
    "    # number of centroids thrown in embedding\n",
    "    n_clusters = 100\n",
    "\n",
    "elseif dataset == \"Internet\"\n",
    "\n",
    "    list = readdlm(\"data/Internet/InternetEList.txt\")\n",
    "    dim0 = size(list)\n",
    "\n",
    "    M_int = zeros(Int64,dim0[1],2)\n",
    "\n",
    "    for i = 1:dim0[1]\n",
    "        for j = 1:2\n",
    "            M_int[i,j] = Int64(list[i,j]) + 1 # index start from 0\n",
    "        end\n",
    "    end\n",
    "\n",
    "    N = maximum([maximum(M_int[:,1]),maximum(M_int[:,2])]) \n",
    "\n",
    "    A0 = spzeros(N,N)\n",
    "    A0 = sparse(M_int[:,1],M_int[:,2],vec(ones(Int64,dim0[1],1)),N,N)\n",
    "    A0 = A0 + A0'\n",
    "    A,_ = largest_component(A0)\n",
    "\n",
    "    A0 = 0\n",
    "    M_int = 0\n",
    "\n",
    "    # number of columns of initial guess\n",
    "    d0 = 50\n",
    "    # number of centroids thrown in embedding\n",
    "    n_clusters = 100\n",
    "\n",
    "elseif dataset == \"CondMat2003\"\n",
    "\n",
    "    list = readdlm(\"data/CondMat2003/CondMat2003EList.txt\")\n",
    "    dim0 = size(list)\n",
    "\n",
    "    M_int = zeros(Int64,dim0[1],2)\n",
    "\n",
    "    for i = 1:dim0[1]\n",
    "        for j = 1:2\n",
    "            M_int[i,j] = Int64(list[i,j]) + 1 # index start from 0\n",
    "        end\n",
    "    end\n",
    "\n",
    "    N = maximum([maximum(M_int[:,1]),maximum(M_int[:,2])])\n",
    "\n",
    "    A0 = spzeros(N,N)\n",
    "    A0 = sparse(M_int[:,1],M_int[:,2],vec(ones(Int64,dim0[1],1)),N,N)\n",
    "    A0 = A0 + A0'\n",
    "    A,_ = largest_component(A0)\n",
    "\n",
    "    A0 = 0\n",
    "    p = 0\n",
    "    M_int = 0\n",
    "\n",
    "    # number of columns of initial guess\n",
    "    d0 = 50\n",
    "    # number of centroids thrown in embedding\n",
    "    n_clusters = 100\n",
    "\n",
    "elseif dataset == \"Pokec\"\n",
    "\n",
    "    list = readdlm(\"data/Pokec/soc-pokec-relationships.txt\")\n",
    "    dim0 = size(list)\n",
    "\n",
    "    M_int = zeros(Int64,dim0[1],2)\n",
    "\n",
    "    for i = 1:dim0[1]\n",
    "        for j = 1:2\n",
    "            M_int[i,j] = Int64(list[i,j]) + 1 # index start from 0\n",
    "        end\n",
    "    end\n",
    "\n",
    "    N = maximum([maximum(M_int[:,1]),maximum(M_int[:,2])])\n",
    "\n",
    "    A0 = spzeros(N,N)\n",
    "    A0 = sparse(M_int[:,1],M_int[:,2],vec(ones(Int64,dim0[1],1)),N,N)\n",
    "    A0 = A0 + A0'\n",
    "    A,_ = largest_component(A0)\n",
    "\n",
    "    A0 = 0\n",
    "    M_int = 0\n",
    "\n",
    "    # number of columns of initial guess\n",
    "    d0 = 50\n",
    "    # number of centroids thrown in embedding\n",
    "    n_clusters = 100\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEP\n",
      " ------- Acc Projected Power Iteration -------\n",
      "The iteration has become stationary after 249 iterations\n",
      " 53.210793 seconds (7.39 k allocations: 22.911 GiB, 8.79% gc time)\n",
      " -------------- Clustering ------- \n",
      "dimension of embedding used for clustering: 50\n",
      "Number of updates: 31\n",
      "Number of communities: 11\n",
      "Modularity: 0.6133639058902282\n",
      " -------------------------------------------- \n",
      "The first 5 squared singular values divided by N : \n",
      "[0.5271776686933346, 0.24814214611090998, 0.16065312735143128, 0.042906295122825656, 0.021114413467887044]\n",
      " -------------------------------------------- \n",
      "166.487245 seconds (23.11 M allocations: 40.280 GiB, 5.70% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.004880901357709299 -0.0023617526361155057 … -0.006288391837689974 0.002015788584667445; -0.008363477093580873 0.009701060253581221 … -0.0040273875070100775 -0.005129423613485266; … ; -0.0002745801859200701 0.0008187338510736741 … -0.0003784434459069713 0.0009359916802831991; 0.0013885365364893017 -0.00016294393416883346 … 0.00011992063986503726 -0.0004341538541014601], [1, 2, 2, 3, 4, 3, 3, 3, 3, 5  …  5, 3, 3, 2, 6, 3, 3, 3, 6, 6], [120.18597306756462, 82.45662376934271, 66.34678356506227, 34.28749752264552, 24.052753044508336, 0.38514994410141057, 0.12739564397158687, 0.06545081729719546, 0.05035843715719228, 0.030325260262961944  …  0.0003840811753376466, 0.000354191516882918, 0.00030895394312118845, 0.000200895544448172, 0.00015782120813048097, 0.0001199197963524278, 0.0001148995708830136, 9.119101720972526e-5, 5.376595029225893e-5, 4.9663774777911216e-5])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(\"\\n\")\n",
    "x_embed,community,singular =  @time sphere_embed_cluster(A, n_it_PPM, t, n_clusters, n_it_vec_part, n_updates, shape, d0)\n",
    "# run twice to get timings without compilation time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
