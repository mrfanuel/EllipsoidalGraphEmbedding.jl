{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MatrixNetworks\n",
    "using Distributions\n",
    "using Gadfly\n",
    "using Clustering\n",
    "using BenchmarkTools, Compat\n",
    "using DelimitedFiles\n",
    "using SparseArrays\n",
    "using LinearAlgebra\n",
    "using SphericalGraphEmbedding\n",
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CondMat2003"
     ]
    }
   ],
   "source": [
    "dataset = \"PolBlogs\"\n",
    "dataset = \"PowerEU\"\n",
    "dataset = \"Facebook\"\n",
    "dataset = \"PowerUS\"\n",
    "dataset = \"HEP\"\n",
    "dataset = \"Internet\"\n",
    "dataset = \"CondMat2003\"\n",
    "#dataset = \"Pokec\"\n",
    "\n",
    "# Initialization\n",
    "A = 0\n",
    "r = 0\n",
    "dim0 = 0\n",
    "community = zeros(Int64,N,1);\n",
    "\n",
    "# nb iterations projected power method\n",
    "n_it_PPM = 30000\n",
    "\n",
    "# relative objective variation\n",
    "t = 1e-10\n",
    "\n",
    "# nb of times vector partition is repeated\n",
    "n_it_vec_part = 100\n",
    "\n",
    "# nb updates vec part\n",
    "n_updates = 100 \n",
    "\n",
    "# shape of embedding\n",
    "shape = \"Ellipsoidal\"\n",
    "\n",
    "# Loading data\n",
    "\n",
    "if dataset == \"PolBlogs\"\n",
    "\n",
    "    A0 = readdlm(\"data/PolBlogs/PolBlogsAdj.txt\")\n",
    "    lab = readdlm(\"data/PolBlogs/LabelsPolBlogs.txt\")\n",
    "\n",
    "    dim0 = size(A0)\n",
    "    A  = zeros(Int64 ,dim0[1],dim0[1])\n",
    "\n",
    "    for i = 1:dim0[1]\n",
    "        for j = 1:dim0[1]\n",
    "            A[i,j] = Int64(A0[i,j])\n",
    "        end\n",
    "    end\n",
    "    A = A+A'\n",
    "    A = sparse(A)\n",
    "\n",
    "    A,p = largest_component(A)\n",
    "    lab = lab[p]\n",
    "    A0 = 0\n",
    "    p = 0 \n",
    "    N = size(A,2)\n",
    "\n",
    "    # number of columns of initial guess\n",
    "    r = 20\n",
    "    # number of centroids thrown in embedding\n",
    "    n_clusters = 100\n",
    "\n",
    "elseif dataset == \"PowerEU\"\n",
    "\n",
    "    list = readdlm(\"data/PowerEU/ElistPowerEU.txt\")\n",
    "    dim0 = size(list)\n",
    "\n",
    "    M_int = zeros(Int64,dim0[1],2)\n",
    "\n",
    "    for i = 1:dim0[1]\n",
    "        for j = 1:2\n",
    "            M_int[i,j] = Int64(list[i,j])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    N = maximum([maximum(M_int[:,1]),maximum(M_int[:,2])])\n",
    "\n",
    "    A0 = spzeros(N,N)\n",
    "    A0 = sparse(M_int[:,1],M_int[:,2],vec(ones(Int64,dim0[1],1)),N,N)\n",
    "    A0 = A0 + A0'\n",
    "    A,p = largest_component(A0)\n",
    "\n",
    "    A0 = 0\n",
    "    p = 0\n",
    "    m = 0\n",
    "\n",
    "    # number of columns of initial guess\n",
    "    r = 50\n",
    "    # number of centroids thrown in embedding\n",
    "    n_clusters = 100\n",
    "\n",
    "elseif dataset == \"Facebook\"\n",
    "\n",
    "    list = readdlm(\"data/Facebook/facebook-gender_edges.txt\")\n",
    "    dim0 = size(list)\n",
    "\n",
    "    M_int = zeros(Int64,dim0[1],2)\n",
    "\n",
    "    for i = 1:dim0[1]\n",
    "        for j = 1:2\n",
    "            M_int[i,j] = Int64(list[i,j]) + 1 # here start at 0\n",
    "        end\n",
    "    end\n",
    "\n",
    "    N = maximum([maximum(M_int[:,1]),maximum(M_int[:,2])]) # here start at 0\n",
    "\n",
    "    A0 = spzeros(N,N)\n",
    "    A0 = sparse(M_int[:,1],M_int[:,2],vec(ones(Int64,dim0[1],1)),N,N)\n",
    "    A0 = A0 + A0'\n",
    "    A,p = largest_component(A0)\n",
    "\n",
    "    A0 = 0\n",
    "    p = 0\n",
    "    m = 0\n",
    "\n",
    "    # number of columns of initial guess\n",
    "    r = 50\n",
    "    # number of centroids thrown in embedding\n",
    "    n_clusters = 100\n",
    "\n",
    "elseif dataset == \"PowerUS\"\n",
    "\n",
    "    list = readdlm(\"data/PowerUS/ElistPowerUS.txt\")\n",
    "    dim0 = size(list)\n",
    "\n",
    "    M_int = zeros(Int64,dim0[1],2)\n",
    "\n",
    "    for i = 1:dim0[1]\n",
    "        for j = 1:2\n",
    "            M_int[i,j] = Int64(list[i,j])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    N = maximum([maximum(M_int[:,1]),maximum(M_int[:,2])])\n",
    "\n",
    "    A0 = spzeros(N,N)\n",
    "    A0 = sparse(M_int[:,1],M_int[:,2],vec(ones(Int64,dim0[1],1)),N,N)\n",
    "    A0 = A0 + A0'\n",
    "    A,p = largest_component(A0)\n",
    "\n",
    "    A0 = 0\n",
    "    p = 0\n",
    "    m = 0\n",
    "\n",
    "    # number of columns of initial guess\n",
    "    r = 50\n",
    "    # number of centroids thrown in embedding\n",
    "    n_clusters = 100\n",
    "\n",
    "elseif dataset == \"HEP\"\n",
    "\n",
    "    list = readdlm(\"data/HEP/hep-year_edges.txt\")\n",
    "    dim0 = size(list)\n",
    "\n",
    "    M_int = zeros(Int64,dim0[1],2)\n",
    "\n",
    "    for i = 1:dim0[1]\n",
    "        for j = 1:2\n",
    "            M_int[i,j] = Int64(list[i,j]) \n",
    "        end\n",
    "    end\n",
    "\n",
    "    N = maximum([maximum(M_int[:,1]),maximum(M_int[:,2])]) \n",
    "\n",
    "    A0 = spzeros(N,N)\n",
    "    A0 = sparse(M_int[:,1],M_int[:,2],vec(ones(Int64,dim0[1],1)),N,N)\n",
    "    A0 = A0 + A0'\n",
    "    A,p = largest_component(A0)\n",
    "\n",
    "    A0 = 0\n",
    "    p = 0\n",
    "    m = 0\n",
    "\n",
    "    # number of columns of initial guess\n",
    "    r = 50\n",
    "    # number of centroids thrown in embedding\n",
    "    n_clusters = 100\n",
    "\n",
    "elseif dataset == \"Wikivote\"\n",
    "\n",
    "    list = readdlm(\"data/Wikivote/wiki-Vote.txt\")\n",
    "    dim0 = size(list)\n",
    "\n",
    "    M_int = zeros(Int64,dim0[1],2)\n",
    "\n",
    "    for i = 1:dim0[1]\n",
    "        for j = 1:2\n",
    "            M_int[i,j] = Int64(list[i,j]) \n",
    "        end\n",
    "    end\n",
    "\n",
    "    N = maximum([maximum(M_int[:,1]),maximum(M_int[:,2])]) \n",
    "\n",
    "    A0 = spzeros(N,N)\n",
    "    A0 = sparse(M_int[:,1],M_int[:,2],vec(ones(Int64,dim0[1],1)),N,N)\n",
    "    A0 = A0 + A0'\n",
    "    A,p = largest_component(A0)\n",
    "\n",
    "    A0 = 0\n",
    "    p = 0\n",
    "    m = 0\n",
    "\n",
    "    # number of columns of initial guess\n",
    "    r = 50\n",
    "    # number of centroids thrown in embedding\n",
    "    n_clusters = 100\n",
    "\n",
    "elseif dataset == \"Internet\"\n",
    "\n",
    "    list = readdlm(\"data/Internet/InternetEList.txt\")\n",
    "    dim0 = size(list)\n",
    "\n",
    "    M_int = zeros(Int64,dim0[1],2)\n",
    "\n",
    "    for i = 1:dim0[1]\n",
    "        for j = 1:2\n",
    "            M_int[i,j] = Int64(list[i,j]) + 1 # index start from 0\n",
    "        end\n",
    "    end\n",
    "\n",
    "    N = maximum([maximum(M_int[:,1]),maximum(M_int[:,2])]) \n",
    "\n",
    "    A0 = spzeros(N,N)\n",
    "    A0 = sparse(M_int[:,1],M_int[:,2],vec(ones(Int64,dim0[1],1)),N,N)\n",
    "    A0 = A0 + A0'\n",
    "    A,p = largest_component(A0)\n",
    "\n",
    "    A0 = 0\n",
    "    p = 0\n",
    "    m = 0\n",
    "\n",
    "    # number of columns of initial guess\n",
    "    r = 50\n",
    "    # number of centroids thrown in embedding\n",
    "    n_clusters = 100\n",
    "\n",
    "elseif dataset == \"CondMat2003\"\n",
    "\n",
    "    list = readdlm(\"data/CondMat2003/CondMat2003EList.txt\")\n",
    "    dim0 = size(list)\n",
    "\n",
    "    M_int = zeros(Int64,dim0[1],2)\n",
    "\n",
    "    for i = 1:dim0[1]\n",
    "        for j = 1:2\n",
    "            M_int[i,j] = Int64(list[i,j]) + 1 # index start from 0\n",
    "        end\n",
    "    end\n",
    "\n",
    "    N = maximum([maximum(M_int[:,1]),maximum(M_int[:,2])])\n",
    "\n",
    "    A0 = spzeros(N,N)\n",
    "    A0 = sparse(M_int[:,1],M_int[:,2],vec(ones(Int64,dim0[1],1)),N,N)\n",
    "    A0 = A0 + A0'\n",
    "    A,p = largest_component(A0)\n",
    "\n",
    "    A0 = 0\n",
    "    p = 0\n",
    "    m = 0\n",
    "\n",
    "    # number of columns of initial guess\n",
    "    r = 50\n",
    "    # number of centroids thrown in embedding\n",
    "    n_clusters = 100\n",
    "\n",
    "elseif dataset == \"Pokec\"\n",
    "\n",
    "    list = readdlm(\"data/Pokec/soc-pokec-relationships.txt\")\n",
    "    dim0 = size(list)\n",
    "\n",
    "    M_int = zeros(Int64,dim0[1],2)\n",
    "\n",
    "    for i = 1:dim0[1]\n",
    "        for j = 1:2\n",
    "            M_int[i,j] = Int64(list[i,j]) + 1 # index start from 0\n",
    "        end\n",
    "    end\n",
    "\n",
    "    N = maximum([maximum(M_int[:,1]),maximum(M_int[:,2])])\n",
    "\n",
    "    A0 = spzeros(N,N)\n",
    "    A0 = sparse(M_int[:,1],M_int[:,2],vec(ones(Int64,dim0[1],1)),N,N)\n",
    "    A0 = A0 + A0'\n",
    "    A,p = largest_component(A0)\n",
    "\n",
    "    A0 = 0\n",
    "    p = 0\n",
    "    m = 0\n",
    "\n",
    "    # number of columns of initial guess\n",
    "    r = 50\n",
    "    # number of centroids thrown in embedding\n",
    "    n_clusters = 100\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------- Acc Projected Power Iteration -------\n",
      "The iteration has become stationary after 290 iterations\n",
      " 32.632579 seconds (8.58 k allocations: 26.811 GiB, 12.70% gc time)\n",
      " -------------- Clustering ------- \n",
      "dimension of embedding used for clustering: 50\n",
      "Number of updates: 21\n",
      "Number of communities: 11\n",
      "Modularity: 0.6205025895121782\n",
      " -------------------------------------------- \n",
      "The first 5 squared singular values divided by N : \n",
      "[0.6105938822972591, 0.14174232371652565, 0.12040875024145256, 0.07550560512521615, 0.029506047492906355]\n",
      " -------------------------------------------- \n",
      "522.724052 seconds (129.68 M allocations: 125.492 GiB, 5.37% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.0044560657114592565 0.00617495920059514 … -0.0072735455118215905 -0.007193259986357211; 0.009319016842885672 0.005720856449299675 … -0.005281275448439841 -0.0033338312391468943; … ; -0.0036459923892575652 -0.0033687470230055277 … -0.0065961534286518375 -0.003332344222272281; -0.0031457641171445854 -0.001103004326906282 … -0.0014791791626605389 -0.002854125486054118], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset)\n",
    "x_embed,community,singular =  @time sphere_embed_cluster(A, n_it_PPM, t, n_clusters, n_it_vec_part, n_updates, shape, r)\n",
    "# run twice to get timings without compilation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
